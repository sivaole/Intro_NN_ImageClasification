{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SVHN_single_grey2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cP4Mcs43Mtp5",
        "colab_type": "text"
      },
      "source": [
        "Fully Connected Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7vKCpWLMljI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np \n",
        "\n",
        "class Linear():\n",
        "    def __init__(self, in_size, out_size):\n",
        "        self.W = np.random.randn(in_size, out_size) * 0.01\n",
        "        self.b = np.zeros((1, out_size))\n",
        "        self.params = [self.W, self.b]\n",
        "        self.gradW = None\n",
        "        self.gradB = None\n",
        "        self.gradInput = None \n",
        "        #self.gamma = np.ones((in_size, 1))\n",
        "        #self.beta = np.zeros((in_size, 1))\n",
        "        #self.eps_norm=1e-8\n",
        "        self.cache = None\n",
        "        self.gamma = None\n",
        "        self.beta = None    \n",
        "        #self.mu = None\n",
        "        #self.var = None    \n",
        "\n",
        "    def forward(self, X):\n",
        "        self.X = X\n",
        "        #self.gamma = np.sqrt(np.var(X))\n",
        "        #self.beta = np.mean(X)    \n",
        "        #self.mu = np.mean(X,axis=0)\n",
        "        #self.var = np.var(X, axis=0)\n",
        "        #X = batchnorm_forward1(X,self.mu,self.var,self.gamma,self.beta)\n",
        "        X, self.cache = batchnorm_forward(X)\n",
        "        self.output = np.dot(X, self.W) + self.b\n",
        "        return self.output\n",
        "\n",
        "    def backward(self, nextgrad):\n",
        "        #nextgrad, dgamma, dbeta = batchnorm_backward1(nextgrad,self.X,self.mu,self.var,self.gamma,self.beta)\n",
        "        nextgrad, dgamma, dbeta = batchnorm_backward(nextgrad,self.cache)\n",
        "        self.gradW = np.dot(self.X.T, nextgrad)\n",
        "        self.gradB = np.sum(nextgrad, axis=0)\n",
        "        self.gradInput = np.dot(nextgrad, self.W.T)\n",
        "        self.gamma = dgamma\n",
        "        self.beta = dbeta\n",
        "        return self.gradInput, [self.gradW, self.gradB]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVbCBkaDBuDX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def batchnorm_forward1(x,mu,var,gamma,beta):\n",
        "  X_norm = (x - mu)/np.sqrt(var + 1e-8)\n",
        "  out = gamma * X_norm + beta\n",
        "  return out\n",
        "  \n",
        "\n",
        "def batchnorm_backward1(dout,x,mu,var,gamma,beta):\n",
        "  X_mu = x - mu\n",
        "  var_inv = 1./np.sqrt(var + 1e-8)\n",
        "  dX_norm = dout * gamma\n",
        "  dvar = np.sum(dX_norm * X_mu,axis=0) * -0.5 * (var + 1e-8)**(-3/2)\n",
        "  dmu = np.sum(dX_norm * -var_inv ,axis=0) + dvar * 1/n_X * np.sum(-2.* X_mu, axis=0)\n",
        "  dX = (dX_norm * var_inv) + (dmu / n_X) + (dvar * 2/n_X * X_mu)\n",
        "  dbeta = np.sum(dout,axis=0)\n",
        "  dgamma = dout * X_norm\n",
        "  return dx, dgamma, dbeta "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPRveDKhz6L6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def batchnorm_forward(x):\n",
        "      N, D = x.shape\n",
        "      gamma = np.sqrt(np.var(x))\n",
        "      beta = np.mean(x)\n",
        "      eps=1e-8      \n",
        "      #step1: calculate mean\n",
        "      mu = 1./N * np.sum(x, axis = 0)\n",
        "      #step2: subtract mean vector of every trainings example  \n",
        "      xmu = x - mu\n",
        "      #step3: following the lower branch - calculation denominator\n",
        "      sq = xmu ** 2\n",
        "      #step4: calculate variance\n",
        "      var = 1./N * np.sum(sq, axis = 0)\n",
        "      #step5: add eps for numerical stability, then sqrt\n",
        "      sqrtvar = np.sqrt(var + eps)\n",
        "      #step6: invert sqrtwar\n",
        "      ivar = 1./sqrtvar\n",
        "      #step7: execute normalization\n",
        "      xhat = xmu * ivar\n",
        "      #step8: Nor the two transformation steps\n",
        "      gammax = gamma * xhat\n",
        "      #step9\n",
        "      out = gammax + beta\n",
        "      #store intermediate\n",
        "      cache = (xhat,gamma,xmu,ivar,sqrtvar,var,eps)\n",
        "      return out, cache\n",
        "  \n",
        "\n",
        "def batchnorm_backward(dout, cache):\n",
        "\n",
        "      #unfold the variables stored in cache\n",
        "      xhat,gamma,xmu,ivar,sqrtvar,var,eps = cache\n",
        "      #get the dimensions of the input/output\n",
        "      N,D = dout.shape\n",
        "      #step9\n",
        "      dbeta = np.sum(dout, axis=0)\n",
        "      dgammax = dout #not necessary, but more understandable\n",
        "      #step8\n",
        "      #dgamma = np.sum(dgammax*xhat, axis=0)\n",
        "      dgamma = np.sum(np.dot(dgammax, xhat), axis=0)      \n",
        "      dxhat = dgammax * gamma\n",
        "      #step7\n",
        "      divar = np.sum(dxhat*xmu, axis=0)\n",
        "      dxmu1 = dxhat * ivar\n",
        "      #step6\n",
        "      dsqrtvar = -1. /(sqrtvar**2) * divar\n",
        "      #step5\n",
        "      dvar = 0.5 * 1. /np.sqrt(var+eps) * dsqrtvar\n",
        "      #step4\n",
        "      dsq = 1. /N * np.ones((N,D)) * dvar\n",
        "      #step3\n",
        "      dxmu2 = 2 * xmu * dsq\n",
        "      #step2\n",
        "      dx1 = (dxmu1 + dxmu2)\n",
        "      dmu = -1 * np.sum(dxmu1+dxmu2, axis=0)\n",
        "      #step1\n",
        "      dx2 = 1. /N * np.ones((N,D)) * dmu\n",
        "      #step0\n",
        "      dx = dx1 + dx2\n",
        "      return dx, dgamma, dbeta "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXsdjbg0OJ0z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Batchnorm():\n",
        "\n",
        "    def __init__(self,):\n",
        "        self.gamma = np.ones((in_size, 1))\n",
        "        self.beta = np.zeros((in_size, 1))\n",
        "        self.eps_norm=1e-8\n",
        "        self.cache = None\n",
        "\n",
        "    def forward(self,x):\n",
        "      N, D = x.shape\n",
        "      #step1: calculate mean\n",
        "      mu = 1./N * np.sum(x, axis = 0)\n",
        "      #step2: subtract mean vector of every trainings example  \n",
        "      xmu = x - mu\n",
        "      #step3: following the lower branch - calculation denominator\n",
        "      sq = xmu ** 2\n",
        "      #step4: calculate variance\n",
        "      var = 1./N * np.sum(sq, axis = 0)\n",
        "      #step5: add eps for numerical stability, then sqrt\n",
        "      sqrtvar = np.sqrt(var + eps)\n",
        "      #step6: invert sqrtwar\n",
        "      ivar = 1./sqrtvar\n",
        "      #step7: execute normalization\n",
        "      xhat = xmu * ivar\n",
        "      #step8: Nor the two transformation steps\n",
        "      gammax = gamma * xhat\n",
        "      #step9\n",
        "      out = gammax + beta\n",
        "      #store intermediate\n",
        "      self.cache = (xhat,gamma,xmu,ivar,sqrtvar,var,eps)\n",
        "      return out, cache\n",
        "\n",
        "    def backward(self,dout):\n",
        "      #unfold the variables stored in cache\n",
        "      xhat,gamma,xmu,ivar,sqrtvar,var,eps = self.cache\n",
        "      #get the dimensions of the input/output\n",
        "      N,D = dout.shape\n",
        "      #step9\n",
        "      dbeta = np.sum(dout, axis=0)\n",
        "      dgammax = dout #not necessary, but more understandable\n",
        "      #step8\n",
        "      dgamma = np.sum(dgammax*xhat, axis=0)\n",
        "      dxhat = dgammax * gamma\n",
        "      #step7\n",
        "      divar = np.sum(dxhat*xmu, axis=0)\n",
        "      dxmu1 = dxhat * ivar\n",
        "      #step6\n",
        "      dsqrtvar = -1. /(sqrtvar**2) * divar\n",
        "      #step5\n",
        "      dvar = 0.5 * 1. /np.sqrt(var+eps) * dsqrtvar\n",
        "      #step4\n",
        "      dsq = 1. /N * np.ones((N,D)) * dvar\n",
        "      #step3\n",
        "      dxmu2 = 2 * xmu * dsq\n",
        "      #step2\n",
        "      dx1 = (dxmu1 + dxmu2)\n",
        "      dmu = -1 * np.sum(dxmu1+dxmu2, axis=0)\n",
        "      #step1\n",
        "      dx2 = 1. /N * np.ones((N,D)) * dmu\n",
        "      #step0\n",
        "      dx = dx1 + dx2\n",
        "      return dx, dgamma, dbeta"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GebK9560Mz2o",
        "colab_type": "text"
      },
      "source": [
        "Activation Layer (ReLU)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dwh0xpL7MoeG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ReLU():\n",
        "    def __init__(self):\n",
        "        self.params = []\n",
        "        self.gradInput = None\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.output = np.maximum(X, 0)\n",
        "        return self.output\n",
        "\n",
        "    def backward(self, nextgrad):\n",
        "        self.gradInput = nextgrad.copy()\n",
        "        self.gradInput[self.output <=0] = 0\n",
        "        return self.gradInput, []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBNmAse5NW-V",
        "colab_type": "text"
      },
      "source": [
        "softmax function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKRaELJfNUZt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def softmax(x):\n",
        "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKIyTzdfNcvo",
        "colab_type": "text"
      },
      "source": [
        "Cross Entropy Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zXPP89zNZw2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CrossEntropy:\n",
        "    def forward(self, X, y):\n",
        "        self.m = y.shape[0]\n",
        "        self.p = softmax(X)\n",
        "        cross_entropy = -np.log(self.p[range(self.m), y]+1e-16)\n",
        "        loss = np.sum(cross_entropy) / self.m\n",
        "        return loss\n",
        "    \n",
        "    def backward(self, X, y):\n",
        "        y_idx = y.argmax()        \n",
        "        grad = softmax(X)\n",
        "        grad[range(self.m), y] -= 1\n",
        "        grad /= self.m\n",
        "        return grad"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXZXvk0GNpBz",
        "colab_type": "text"
      },
      "source": [
        "Loading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ukC6bpZOpxb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "000516fe-8130-4242-a583-27f573620037"
      },
      "source": [
        "# mount your drive to use the files on your notebook\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37CA6wRlNlui",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "f488d8cc-10e8-4ef7-8c83-e93bc3bbcb72"
      },
      "source": [
        "import h5py\n",
        "from keras.datasets import mnist\n",
        "from keras.utils import np_utils\n",
        "\n",
        "# Open the file as readonly\n",
        "h5f = h5py.File('/content/drive/My Drive/SVHN_single_grey1.h5', 'r')\n",
        "# Load the training, test and validation set\n",
        "X_train = h5f['X_train'][:]\n",
        "y_train = h5f['y_train'][:]\n",
        "X_test = h5f['X_test'][:]\n",
        "y_test = h5f['y_test'][:]\n",
        "# Close this file\n",
        "h5f.close()\n",
        "\n",
        "# reshaping the data at hand\n",
        "X_train = X_train.reshape(X_train.shape[0], 1024)\n",
        "print (X_train.shape)\n",
        "X_test = X_test.reshape(X_test.shape[0], 1024)\n",
        "print (X_test.shape)\n",
        "\n",
        "# # normalize inputs from 0-255 to 0-1\n",
        "X_train = X_train / 255.0\n",
        "X_test = X_test / 255.0\n",
        "\n",
        "X_val = X_test\n",
        "y_val = y_test\n",
        "\n",
        "print (y_train.shape)\n",
        "print (y_test.shape)\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(42000, 1024)\n",
            "(18000, 1024)\n",
            "(42000,)\n",
            "(18000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Avu_5sPFOb9S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 95
        },
        "outputId": "aaa737ef-62ca-46a8-e2a8-00432841b96a"
      },
      "source": [
        "# fix random seed for reproducibility\n",
        "seed = 7\n",
        "np.random.seed(seed)\n",
        "# visualizing the first 10 images in the dataset and their labels\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(10, 1))\n",
        "for i in range(10):\n",
        "    plt.subplot(1, 10, i+1)\n",
        "    plt.imshow(X_train[i].reshape(32, 32), cmap=\"gray\")\n",
        "    plt.axis('off')\n",
        "plt.show()\n",
        "print('label for each of the above image: %s' % (y_train[0:10]))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjwAAAA9CAYAAACpzLMWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhqM4BwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 720x72 with 10 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "label for each of the above image: [2 6 7 4 4 0 3 0 7 3]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76WV_zTuO5J2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NN():\n",
        "    def __init__(self, lossfunc=CrossEntropy(), mode='train'):\n",
        "        self.params = []\n",
        "        self.layers = []\n",
        "        self.loss_func = lossfunc\n",
        "        self.grads = []\n",
        "        self.mode = mode\n",
        "        \n",
        "    def add_layer(self, layer):\n",
        "        self.layers.append(layer)\n",
        "        self.params.append(layer.params)\n",
        "\n",
        "    def forward(self, X):\n",
        "        for layer in self.layers:\n",
        "            X = layer.forward(X)\n",
        "        return X\n",
        "    \n",
        "    def backward(self, nextgrad):\n",
        "        self.clear_grad_param()\n",
        "        for layer in reversed(self.layers):\n",
        "            nextgrad, grad = layer.backward(nextgrad)\n",
        "            self.grads.append(grad)\n",
        "        return self.grads\n",
        "    \n",
        "    def train_step(self, X, y):\n",
        "        #out = self.forward(X)\n",
        "        #loss = self.loss_func.forward(out,y)\n",
        "        #nextgrad = self.loss_func.backward(out,y)\n",
        "        #grads = self.backward(nextgrad)\n",
        "        #return loss, grads\n",
        "        out = self.forward(X)\n",
        "        loss = self.loss_func.forward(out,y)  + ((Lambda / (2 * y.shape[0])) * np.sum([np.sum(w**2) for w in self.params[0][0]]))\n",
        "        nextgrad = self.loss_func.backward(out,y) + ((Lambda/y.shape[0]) * np.sum([np.sum(w) for w in self.params[0][0]]))\n",
        "        grads = self.backward(nextgrad)\n",
        "        return loss, grads\n",
        "    \n",
        "    def predict(self, X):\n",
        "        X = self.forward(X)\n",
        "        p = softmax(X)\n",
        "        return np.argmax(p, axis=1)\n",
        "    \n",
        "    def predict_scores(self, X):\n",
        "        X = self.forward(X)\n",
        "        p = softmax(X)\n",
        "        return p\n",
        "    \n",
        "    def clear_grad_param(self):\n",
        "        self.grads = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kywe446PTSZ",
        "colab_type": "text"
      },
      "source": [
        "update function (SGD with momentum)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQ_qsV0ePPq9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def update_params(velocity, params, grads, learning_rate=0.01, mu=0.9):\n",
        "    for v, p, g, in zip(velocity, params, reversed(grads)):\n",
        "        for i in range(len(g)):\n",
        "            v[i] = mu * v[i] - learning_rate * g[i]\n",
        "            p[i] += v[i]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5TMEruPPZeo",
        "colab_type": "text"
      },
      "source": [
        "minibatches (both the datapoint and the corresponding label)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWU3KrUoPWjA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get minibatches\n",
        "def minibatch(X, y, minibatch_size):\n",
        "    n = X.shape[0]\n",
        "    minibatches = []\n",
        "    permutation = np.random.permutation(X.shape[0])\n",
        "    X = X[permutation]\n",
        "    y = y[permutation]\n",
        "    \n",
        "    for i in range(0, n , minibatch_size):\n",
        "        X_batch = X[i:i + minibatch_size, :]\n",
        "        y_batch = y[i:i + minibatch_size, ]\n",
        "        minibatches.append((X_batch, y_batch))\n",
        "        \n",
        "    return minibatches"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIIV56W3PcKa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sgd(net, X_train, y_train, minibatch_size, epoch, learning_rate, mu=0.9, X_val=None, y_val=None, Lambda=0):\n",
        "    val_loss_epoch = []\n",
        "    minibatches = minibatch(X_train, y_train, minibatch_size)\n",
        "    minibatches_val = minibatch(X_val, y_val, minibatch_size)\n",
        "\n",
        "    \n",
        "    for i in range(epoch):\n",
        "        loss_batch = []\n",
        "        val_loss_batch = []\n",
        "        velocity = []\n",
        "        for param_layer in net.params:\n",
        "            p = [np.zeros_like(param) for param in list(param_layer)]\n",
        "            velocity.append(p)\n",
        "            \n",
        "        # iterate over mini batches\n",
        "        for X_mini, y_mini in minibatches:\n",
        "            loss, grads = net.train_step(X_mini, y_mini)\n",
        "            loss_batch.append(loss)\n",
        "            update_params(velocity, net.params, grads, learning_rate=learning_rate, mu=mu)\n",
        "\n",
        "        for X_mini_val, y_mini_val in minibatches_val:\n",
        "            val_loss, _ = net.train_step(X_mini, y_mini)\n",
        "            val_loss_batch.append(val_loss)\n",
        "        \n",
        "        # accuracy of model at end of epoch after all mini batch updates\n",
        "        m_train = X_train.shape[0]\n",
        "        m_val = X_val.shape[0]\n",
        "        y_train_pred = np.array([], dtype=\"int64\")\n",
        "        y_val_pred = np.array([], dtype=\"int64\")\n",
        "        y_train1 = []\n",
        "        y_vall = []\n",
        "        for i in range(0, m_train, minibatch_size):\n",
        "            X_tr = X_train[i:i + minibatch_size, : ]\n",
        "            y_tr = y_train[i:i + minibatch_size,]\n",
        "            y_train1 = np.append(y_train1, y_tr)\n",
        "            y_train_pred = np.append(y_train_pred, net.predict(X_tr))\n",
        "\n",
        "        for i in range(0, m_val, minibatch_size):\n",
        "            X_va = X_val[i:i + minibatch_size, : ]\n",
        "            y_va = y_val[i:i + minibatch_size,]\n",
        "            y_vall = np.append(y_vall, y_va)\n",
        "            y_val_pred = np.append(y_val_pred, net.predict(X_va))\n",
        "            \n",
        "        train_acc = check_accuracy(y_train1, y_train_pred)\n",
        "        val_acc = check_accuracy(y_vall, y_val_pred)\n",
        "\n",
        "        mean_train_loss = sum(loss_batch) / float(len(loss_batch))\n",
        "        mean_val_loss = sum(val_loss_batch) / float(len(val_loss_batch))\n",
        "        \n",
        "        val_loss_epoch.append(mean_val_loss)\n",
        "        print(\"Loss = {0} | Training Accuracy = {1} | Val Loss = {2} | Val Accuracy = {3}\".format(mean_train_loss, train_acc, mean_val_loss, val_acc))\n",
        "    return net, val_acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bisMIz1fPfj1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def check_accuracy(y_true, y_pred):\n",
        "    return np.mean(y_pred == y_true)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VX7A_ZXBRvfw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "99c187f1-5678-4c67-e64a-9bb7c1fcf77c"
      },
      "source": [
        "print (X_train.shape[1])"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1024\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6klV3ZmSPiWL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from random import shuffle\n",
        "\n",
        "## input size\n",
        "input_dim = X_train.shape[1]\n",
        "\n",
        "## hyperparameters\n",
        "#iterations = 10\n",
        "#learning_rate = 1e4\n",
        "#learning_rate = 0.1\n",
        "#hidden_nodes = 32\n",
        "#output_nodes = 10\n",
        "\n",
        "## define neural net\n",
        "#nn = NN()\n",
        "#nn.add_layer(Linear(input_dim, hidden_nodes))\n",
        "#nn.add_layer(ReLU())\n",
        "#nn.add_layer(Linear(hidden_nodes, output_nodes))\n",
        "\n",
        "#nn = train(nn, X_train , y_train, minibatch_size=200, epoch=10, \\\n",
        " #          learning_rate=learning_rate, X_val=X_val, y_val=y_val)\n",
        "\n",
        "\n",
        "def train_and_test_loop(iterations, lr, Lambda):\n",
        "    ## hyperparameters\n",
        "    iterations = iterations\n",
        "    learning_rate = lr\n",
        "    hidden_nodes = 32\n",
        "    output_nodes = 10\n",
        "\n",
        "    ## define neural net\n",
        "    nn = NN()\n",
        "    nn.add_layer(Linear(input_dim, hidden_nodes))\n",
        "    nn.add_layer(ReLU())\n",
        "    nn.add_layer(Linear(hidden_nodes, output_nodes))\n",
        "\n",
        "    nn, val_acc = sgd(nn, X_train , y_train, minibatch_size=200, epoch=iterations, learning_rate=learning_rate,\\\n",
        "                      X_val=X_val, y_val=y_val, Lambda=Lambda)\n",
        "    return val_acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93z9H4BnQt6D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "57ce61e9-7a35-4dcc-e499-c2dc83d5524a"
      },
      "source": [
        "lr = 0.00001\n",
        "Lambda = 0\n",
        "train_and_test_loop(1, lr, Lambda)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss = 2.3026079139590223 | Training Accuracy = 0.09973809523809524 | Val Loss = 2.3027168487501597 | Val Accuracy = 0.10061111111111111\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.10061111111111111"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3Stz-aQaNrl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "af8cdc38-bbd4-4391-9c61-c49589014008"
      },
      "source": [
        "lr = 0.00001\n",
        "Lambda = 1e3\n",
        "train_and_test_loop(1, lr, Lambda)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss = 3.7407743935257066e+23 | Training Accuracy = 0.09966666666666667 | Val Loss = 4.333849703130508e+23 | Val Accuracy = 0.10077777777777777\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.10077777777777777"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yieK4JoaamgL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c65221e4-8136-4c23-bc14-6b32e2d7ef9d"
      },
      "source": [
        "lr = 1e-7\n",
        "Lambda = 1e-7\n",
        "train_and_test_loop(100, lr, Lambda)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss = 2.3025648708843995 | Training Accuracy = 0.10916666666666666 | Val Loss = 2.3023816158378163 | Val Accuracy = 0.10922222222222222\n",
            "Loss = 2.302564865829785 | Training Accuracy = 0.10916666666666666 | Val Loss = 2.3023816067897083 | Val Accuracy = 0.10927777777777778\n",
            "Loss = 2.302564860774827 | Training Accuracy = 0.10916666666666666 | Val Loss = 2.3023815977280457 | Val Accuracy = 0.10927777777777778\n",
            "Loss = 2.3025648557188916 | Training Accuracy = 0.10916666666666666 | Val Loss = 2.3023815886633945 | Val Accuracy = 0.10927777777777778\n",
            "Loss = 2.3025648506600067 | Training Accuracy = 0.10914285714285714 | Val Loss = 2.30238157962983 | Val Accuracy = 0.10933333333333334\n",
            "Loss = 2.302564845597162 | Training Accuracy = 0.10914285714285714 | Val Loss = 2.302381570596103 | Val Accuracy = 0.10938888888888888\n",
            "Loss = 2.3025648405341497 | Training Accuracy = 0.10916666666666666 | Val Loss = 2.3023815615619934 | Val Accuracy = 0.10938888888888888\n",
            "Loss = 2.3025648354729595 | Training Accuracy = 0.10921428571428571 | Val Loss = 2.3023815525284093 | Val Accuracy = 0.10938888888888888\n",
            "Loss = 2.302564830414001 | Training Accuracy = 0.10921428571428571 | Val Loss = 2.3023815435001045 | Val Accuracy = 0.10938888888888888\n",
            "Loss = 2.302564825356103 | Training Accuracy = 0.10921428571428571 | Val Loss = 2.3023815344942 | Val Accuracy = 0.10938888888888888\n",
            "Loss = 2.3025648203001783 | Training Accuracy = 0.10919047619047619 | Val Loss = 2.3023815255129993 | Val Accuracy = 0.10944444444444444\n",
            "Loss = 2.302564815246982 | Training Accuracy = 0.10916666666666666 | Val Loss = 2.3023815165228627 | Val Accuracy = 0.10938888888888888\n",
            "Loss = 2.30256481019472 | Training Accuracy = 0.10919047619047619 | Val Loss = 2.3023815075377656 | Val Accuracy = 0.10938888888888888\n",
            "Loss = 2.302564805141559 | Training Accuracy = 0.10919047619047619 | Val Loss = 2.302381498550963 | Val Accuracy = 0.10938888888888888\n",
            "Loss = 2.302564800090425 | Training Accuracy = 0.10916666666666666 | Val Loss = 2.30238148956087 | Val Accuracy = 0.10938888888888888\n",
            "Loss = 2.3025647950411763 | Training Accuracy = 0.10919047619047619 | Val Loss = 2.3023814805739766 | Val Accuracy = 0.10938888888888888\n",
            "Loss = 2.3025647899899875 | Training Accuracy = 0.10919047619047619 | Val Loss = 2.3023814715932924 | Val Accuracy = 0.10938888888888888\n",
            "Loss = 2.3025647849354343 | Training Accuracy = 0.10921428571428571 | Val Loss = 2.3023814625818484 | Val Accuracy = 0.10938888888888888\n",
            "Loss = 2.3025647798812994 | Training Accuracy = 0.10921428571428571 | Val Loss = 2.3023814535748603 | Val Accuracy = 0.10944444444444444\n",
            "Loss = 2.3025647748249787 | Training Accuracy = 0.10926190476190477 | Val Loss = 2.3023814445820414 | Val Accuracy = 0.10944444444444444\n",
            "Loss = 2.3025647697666085 | Training Accuracy = 0.10926190476190477 | Val Loss = 2.3023814355906036 | Val Accuracy = 0.10944444444444444\n",
            "Loss = 2.3025647647076335 | Training Accuracy = 0.10926190476190477 | Val Loss = 2.3023814266004474 | Val Accuracy = 0.10944444444444444\n",
            "Loss = 2.302564759649016 | Training Accuracy = 0.10926190476190477 | Val Loss = 2.3023814175985247 | Val Accuracy = 0.10944444444444444\n",
            "Loss = 2.30256475459255 | Training Accuracy = 0.10928571428571429 | Val Loss = 2.302381408621613 | Val Accuracy = 0.10944444444444444\n",
            "Loss = 2.3025647495362644 | Training Accuracy = 0.10926190476190477 | Val Loss = 2.3023813996410047 | Val Accuracy = 0.10933333333333334\n",
            "Loss = 2.3025647444804354 | Training Accuracy = 0.10928571428571429 | Val Loss = 2.302381390656675 | Val Accuracy = 0.10927777777777778\n",
            "Loss = 2.302564739425172 | Training Accuracy = 0.10930952380952381 | Val Loss = 2.3023813816688303 | Val Accuracy = 0.10922222222222222\n",
            "Loss = 2.3025647343706965 | Training Accuracy = 0.10930952380952381 | Val Loss = 2.3023813726932194 | Val Accuracy = 0.10922222222222222\n",
            "Loss = 2.3025647293159084 | Training Accuracy = 0.10933333333333334 | Val Loss = 2.302381363702663 | Val Accuracy = 0.10922222222222222\n",
            "Loss = 2.3025647242642715 | Training Accuracy = 0.10935714285714286 | Val Loss = 2.302381354706254 | Val Accuracy = 0.10922222222222222\n",
            "Loss = 2.3025647192131578 | Training Accuracy = 0.10935714285714286 | Val Loss = 2.302381345717235 | Val Accuracy = 0.10922222222222222\n",
            "Loss = 2.3025647141604035 | Training Accuracy = 0.10940476190476191 | Val Loss = 2.302381336726055 | Val Accuracy = 0.10922222222222222\n",
            "Loss = 2.3025647091064965 | Training Accuracy = 0.10935714285714286 | Val Loss = 2.3023813277335154 | Val Accuracy = 0.10922222222222222\n",
            "Loss = 2.302564704051926 | Training Accuracy = 0.10933333333333334 | Val Loss = 2.3023813187761824 | Val Accuracy = 0.10916666666666666\n",
            "Loss = 2.302564698995262 | Training Accuracy = 0.10933333333333334 | Val Loss = 2.302381309797992 | Val Accuracy = 0.10922222222222222\n",
            "Loss = 2.3025646939390287 | Training Accuracy = 0.10935714285714286 | Val Loss = 2.3023813008180105 | Val Accuracy = 0.10922222222222222\n",
            "Loss = 2.3025646888825353 | Training Accuracy = 0.10935714285714286 | Val Loss = 2.302381291842201 | Val Accuracy = 0.10922222222222222\n",
            "Loss = 2.302564683825071 | Training Accuracy = 0.10933333333333334 | Val Loss = 2.302381282865335 | Val Accuracy = 0.10922222222222222\n",
            "Loss = 2.302564678767251 | Training Accuracy = 0.10933333333333334 | Val Loss = 2.302381273893494 | Val Accuracy = 0.10911111111111112\n",
            "Loss = 2.302564673708971 | Training Accuracy = 0.10933333333333334 | Val Loss = 2.3023812648982784 | Val Accuracy = 0.10911111111111112\n",
            "Loss = 2.3025646686487273 | Training Accuracy = 0.10935714285714286 | Val Loss = 2.302381255905829 | Val Accuracy = 0.10905555555555556\n",
            "Loss = 2.302564663588372 | Training Accuracy = 0.10935714285714286 | Val Loss = 2.302381246911645 | Val Accuracy = 0.10905555555555556\n",
            "Loss = 2.3025646585271105 | Training Accuracy = 0.10935714285714286 | Val Loss = 2.30238123787094 | Val Accuracy = 0.10911111111111112\n",
            "Loss = 2.302564653463377 | Training Accuracy = 0.10935714285714286 | Val Loss = 2.3023812288522634 | Val Accuracy = 0.10911111111111112\n",
            "Loss = 2.3025646483966664 | Training Accuracy = 0.10935714285714286 | Val Loss = 2.302381219844447 | Val Accuracy = 0.10911111111111112\n",
            "Loss = 2.302564643331836 | Training Accuracy = 0.10933333333333334 | Val Loss = 2.3023812108191 | Val Accuracy = 0.10905555555555556\n",
            "Loss = 2.3025646382675986 | Training Accuracy = 0.10938095238095238 | Val Loss = 2.302381201791597 | Val Accuracy = 0.10911111111111112\n",
            "Loss = 2.30256463320204 | Training Accuracy = 0.10940476190476191 | Val Loss = 2.3023811927553552 | Val Accuracy = 0.10911111111111112\n",
            "Loss = 2.302564628135295 | Training Accuracy = 0.10942857142857143 | Val Loss = 2.3023811837237527 | Val Accuracy = 0.10911111111111112\n",
            "Loss = 2.3025646230655985 | Training Accuracy = 0.10940476190476191 | Val Loss = 2.3023811746902543 | Val Accuracy = 0.10911111111111112\n",
            "Loss = 2.302564617995322 | Training Accuracy = 0.10938095238095238 | Val Loss = 2.302381165656842 | Val Accuracy = 0.10916666666666666\n",
            "Loss = 2.3025646129255537 | Training Accuracy = 0.10938095238095238 | Val Loss = 2.302381156631837 | Val Accuracy = 0.10916666666666666\n",
            "Loss = 2.3025646078549613 | Training Accuracy = 0.10938095238095238 | Val Loss = 2.3023811476057854 | Val Accuracy = 0.10922222222222222\n",
            "Loss = 2.302564602784504 | Training Accuracy = 0.10938095238095238 | Val Loss = 2.3023811385707598 | Val Accuracy = 0.10927777777777778\n",
            "Loss = 2.3025645977136278 | Training Accuracy = 0.10938095238095238 | Val Loss = 2.302381129541116 | Val Accuracy = 0.10916666666666666\n",
            "Loss = 2.3025645926431495 | Training Accuracy = 0.10935714285714286 | Val Loss = 2.302381120477664 | Val Accuracy = 0.10916666666666666\n",
            "Loss = 2.3025645875751026 | Training Accuracy = 0.10935714285714286 | Val Loss = 2.302381111404337 | Val Accuracy = 0.10916666666666666\n",
            "Loss = 2.302564582505541 | Training Accuracy = 0.10935714285714286 | Val Loss = 2.3023811023390195 | Val Accuracy = 0.10905555555555556\n",
            "Loss = 2.3025645774331664 | Training Accuracy = 0.10933333333333334 | Val Loss = 2.3023810932628503 | Val Accuracy = 0.10911111111111112\n",
            "Loss = 2.302564572359801 | Training Accuracy = 0.10935714285714286 | Val Loss = 2.3023810841856487 | Val Accuracy = 0.10911111111111112\n",
            "Loss = 2.3025645672869697 | Training Accuracy = 0.10935714285714286 | Val Loss = 2.302381075105096 | Val Accuracy = 0.10911111111111112\n",
            "Loss = 2.302564562212359 | Training Accuracy = 0.10935714285714286 | Val Loss = 2.3023810660251995 | Val Accuracy = 0.10911111111111112\n",
            "Loss = 2.302564557138264 | Training Accuracy = 0.10938095238095238 | Val Loss = 2.3023810569354954 | Val Accuracy = 0.10911111111111112\n",
            "Loss = 2.302564552066752 | Training Accuracy = 0.10938095238095238 | Val Loss = 2.3023810478495 | Val Accuracy = 0.10905555555555556\n",
            "Loss = 2.302564546994486 | Training Accuracy = 0.10938095238095238 | Val Loss = 2.302381038756218 | Val Accuracy = 0.10911111111111112\n",
            "Loss = 2.3025645419237835 | Training Accuracy = 0.10935714285714286 | Val Loss = 2.302381029632071 | Val Accuracy = 0.10905555555555556\n",
            "Loss = 2.3025645368575685 | Training Accuracy = 0.10938095238095238 | Val Loss = 2.3023810205082778 | Val Accuracy = 0.10905555555555556\n",
            "Loss = 2.3025645317905408 | Training Accuracy = 0.10938095238095238 | Val Loss = 2.3023810113836234 | Val Accuracy = 0.10905555555555556\n",
            "Loss = 2.3025645267246375 | Training Accuracy = 0.10935714285714286 | Val Loss = 2.3023810022335534 | Val Accuracy = 0.10905555555555556\n",
            "Loss = 2.302564521661326 | Training Accuracy = 0.10935714285714286 | Val Loss = 2.302380993086225 | Val Accuracy = 0.10911111111111112\n",
            "Loss = 2.3025645165982147 | Training Accuracy = 0.10938095238095238 | Val Loss = 2.3023809839561697 | Val Accuracy = 0.10911111111111112\n",
            "Loss = 2.302564511534751 | Training Accuracy = 0.10938095238095238 | Val Loss = 2.3023809748271185 | Val Accuracy = 0.10905555555555556\n",
            "Loss = 2.302564506471994 | Training Accuracy = 0.10935714285714286 | Val Loss = 2.3023809657215675 | Val Accuracy = 0.109\n",
            "Loss = 2.3025645014104392 | Training Accuracy = 0.10935714285714286 | Val Loss = 2.3023809566166737 | Val Accuracy = 0.109\n",
            "Loss = 2.302564496348714 | Training Accuracy = 0.10938095238095238 | Val Loss = 2.302380947513711 | Val Accuracy = 0.109\n",
            "Loss = 2.3025644912871828 | Training Accuracy = 0.10938095238095238 | Val Loss = 2.3023809384150433 | Val Accuracy = 0.109\n",
            "Loss = 2.3025644862268004 | Training Accuracy = 0.10940476190476191 | Val Loss = 2.3023809293594293 | Val Accuracy = 0.109\n",
            "Loss = 2.3025644811648234 | Training Accuracy = 0.10938095238095238 | Val Loss = 2.3023809202948367 | Val Accuracy = 0.109\n",
            "Loss = 2.302564476102212 | Training Accuracy = 0.10933333333333334 | Val Loss = 2.3023809112339593 | Val Accuracy = 0.109\n",
            "Loss = 2.302564471037614 | Training Accuracy = 0.10933333333333334 | Val Loss = 2.3023809021808863 | Val Accuracy = 0.10894444444444444\n",
            "Loss = 2.302564465974474 | Training Accuracy = 0.10933333333333334 | Val Loss = 2.302380893123796 | Val Accuracy = 0.109\n",
            "Loss = 2.302564460911175 | Training Accuracy = 0.10933333333333334 | Val Loss = 2.3023808840765985 | Val Accuracy = 0.10894444444444444\n",
            "Loss = 2.302564455846679 | Training Accuracy = 0.10935714285714286 | Val Loss = 2.302380875030373 | Val Accuracy = 0.10894444444444444\n",
            "Loss = 2.3025644507822207 | Training Accuracy = 0.10933333333333334 | Val Loss = 2.302380865983032 | Val Accuracy = 0.10888888888888888\n",
            "Loss = 2.3025644457181222 | Training Accuracy = 0.10935714285714286 | Val Loss = 2.302380856931249 | Val Accuracy = 0.10877777777777778\n",
            "Loss = 2.302564440654129 | Training Accuracy = 0.10935714285714286 | Val Loss = 2.30238084788081 | Val Accuracy = 0.10877777777777778\n",
            "Loss = 2.3025644355896757 | Training Accuracy = 0.10935714285714286 | Val Loss = 2.3023808388254277 | Val Accuracy = 0.10883333333333334\n",
            "Loss = 2.3025644305257087 | Training Accuracy = 0.10938095238095238 | Val Loss = 2.3023808297769772 | Val Accuracy = 0.10883333333333334\n",
            "Loss = 2.302564425462824 | Training Accuracy = 0.10938095238095238 | Val Loss = 2.3023808207297254 | Val Accuracy = 0.10883333333333334\n",
            "Loss = 2.302564420396352 | Training Accuracy = 0.10942857142857143 | Val Loss = 2.302380811724057 | Val Accuracy = 0.10883333333333334\n",
            "Loss = 2.3025644153262483 | Training Accuracy = 0.10942857142857143 | Val Loss = 2.30238080270518 | Val Accuracy = 0.10883333333333334\n",
            "Loss = 2.3025644102573586 | Training Accuracy = 0.10940476190476191 | Val Loss = 2.302380793749649 | Val Accuracy = 0.10877777777777778\n",
            "Loss = 2.3025644051902634 | Training Accuracy = 0.10942857142857143 | Val Loss = 2.3023807847993116 | Val Accuracy = 0.10877777777777778\n",
            "Loss = 2.3025644001250036 | Training Accuracy = 0.10942857142857143 | Val Loss = 2.3023807758360055 | Val Accuracy = 0.10872222222222222\n",
            "Loss = 2.3025643950589134 | Training Accuracy = 0.10940476190476191 | Val Loss = 2.3023807668619796 | Val Accuracy = 0.10872222222222222\n",
            "Loss = 2.3025643899933512 | Training Accuracy = 0.10938095238095238 | Val Loss = 2.302380757879632 | Val Accuracy = 0.10872222222222222\n",
            "Loss = 2.302564384928006 | Training Accuracy = 0.10940476190476191 | Val Loss = 2.3023807488942243 | Val Accuracy = 0.10872222222222222\n",
            "Loss = 2.3025643798635476 | Training Accuracy = 0.10945238095238095 | Val Loss = 2.302380739909418 | Val Accuracy = 0.10872222222222222\n",
            "Loss = 2.3025643747994557 | Training Accuracy = 0.10945238095238095 | Val Loss = 2.302380730925788 | Val Accuracy = 0.10872222222222222\n",
            "Loss = 2.3025643697352542 | Training Accuracy = 0.10945238095238095 | Val Loss = 2.302380721947438 | Val Accuracy = 0.10866666666666666\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.10866666666666666"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vx5e3L7ca11s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "feaa9b49-7684-44fa-de57-52fc51af390c"
      },
      "source": [
        "lr = 1e6\n",
        "Lambda = 1e-7\n",
        "train_and_test_loop(100, lr, Lambda)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss = 1.6584360070330646e+89 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8114316078192855e+89 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.811431607819288e+89 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8114316078192855e+89 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.811431607819288e+89 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8114316078192855e+89 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.811431607819288e+89 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8114316078192855e+89 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.811431607819288e+89 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8114316078192855e+89 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.811431607819288e+89 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8114316078192855e+89 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.811431607819288e+89 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8114316078192855e+89 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.811431607819288e+89 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8114316078192855e+89 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.811431607819288e+89 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8114316078192855e+89 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.811431607819288e+89 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8114316078192855e+89 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.811431607819288e+89 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8114316078192855e+89 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.811431607819288e+89 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8114316078192855e+89 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.811431607819288e+89 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8114316078192855e+89 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.811431607819288e+89 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8114316078192855e+89 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.811431607819288e+89 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8114316078192855e+89 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.811431607819288e+89 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8114316078192855e+89 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.811431607819288e+89 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8114316078192855e+89 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.811431607819288e+89 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8114316078192855e+89 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.811431607819288e+89 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8114316078192855e+89 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.811431607819288e+89 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8114316078192855e+89 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.811431607819288e+89 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8114316078192855e+89 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.811431607819288e+89 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8114316078192855e+89 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.811431607819288e+89 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8114316078192855e+89 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.811431607819288e+89 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8114316078192855e+89 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.811431607819288e+89 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8114316078192855e+89 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.811431607819288e+89 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8114316078192855e+89 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.811431607819288e+89 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8114316078192855e+89 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.811431607819288e+89 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8114316078192855e+89 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.811431607819288e+89 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8114316078192855e+89 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.811431607819288e+89 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8114316078192855e+89 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.811431607819288e+89 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8114316078192855e+89 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.811431607819288e+89 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8114316078192855e+89 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.811431607819288e+89 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8114316078192855e+89 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.811431607819288e+89 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8114316078192855e+89 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.811431607819288e+89 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8114316078192855e+89 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.811431607819288e+89 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8114316078192855e+89 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.811431607819288e+89 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8114316078192855e+89 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.811431607819288e+89 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8114316078192855e+89 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.811431607819288e+89 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8114316078192855e+89 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.811431607819288e+89 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8114316078192855e+89 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.811431607819288e+89 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8114316078192855e+89 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.811431607819288e+89 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8114316078192855e+89 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.811431607819288e+89 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8114316078192855e+89 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.811431607819288e+89 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8114316078192855e+89 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.811431607819288e+89 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8114316078192855e+89 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.811431607819288e+89 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8114316078192855e+89 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.811431607819288e+89 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8114316078192855e+89 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.811431607819288e+89 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8114316078192855e+89 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.811431607819288e+89 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8114316078192855e+89 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.811431607819288e+89 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8114316078192855e+89 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.811431607819288e+89 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8114316078192855e+89 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.811431607819288e+89 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8114316078192855e+89 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.811431607819288e+89 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8114316078192855e+89 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.811431607819288e+89 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8114316078192855e+89 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.811431607819288e+89 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8114316078192855e+89 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.811431607819288e+89 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8114316078192855e+89 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.811431607819288e+89 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8114316078192855e+89 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.811431607819288e+89 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8114316078192855e+89 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.811431607819288e+89 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8114316078192855e+89 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.811431607819288e+89 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8114316078192855e+89 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.811431607819288e+89 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8114316078192855e+89 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.811431607819288e+89 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8114316078192855e+89 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.811431607819288e+89 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8114316078192855e+89 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.811431607819288e+89 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8114316078192855e+89 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.811431607819288e+89 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8114316078192855e+89 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.811431607819288e+89 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8114316078192855e+89 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.811431607819288e+89 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8114316078192855e+89 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.811431607819288e+89 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8114316078192855e+89 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.811431607819288e+89 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8114316078192855e+89 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.811431607819288e+89 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8114316078192855e+89 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.811431607819288e+89 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8114316078192855e+89 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.811431607819288e+89 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8114316078192855e+89 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.811431607819288e+89 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8114316078192855e+89 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.811431607819288e+89 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8114316078192855e+89 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.811431607819288e+89 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8114316078192855e+89 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.811431607819288e+89 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8114316078192855e+89 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.811431607819288e+89 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8114316078192855e+89 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.811431607819288e+89 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8114316078192855e+89 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.811431607819288e+89 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8114316078192855e+89 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.811431607819288e+89 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8114316078192855e+89 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.811431607819288e+89 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8114316078192855e+89 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.811431607819288e+89 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8114316078192855e+89 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.811431607819288e+89 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8114316078192855e+89 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.811431607819288e+89 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8114316078192855e+89 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.811431607819288e+89 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8114316078192855e+89 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.811431607819288e+89 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8114316078192855e+89 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.811431607819288e+89 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8114316078192855e+89 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.811431607819288e+89 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8114316078192855e+89 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.811431607819288e+89 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8114316078192855e+89 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.811431607819288e+89 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8114316078192855e+89 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.811431607819288e+89 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8114316078192855e+89 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.811431607819288e+89 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8114316078192855e+89 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.811431607819288e+89 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8114316078192855e+89 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.811431607819288e+89 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8114316078192855e+89 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.811431607819288e+89 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8114316078192855e+89 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.811431607819288e+89 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8114316078192855e+89 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.811431607819288e+89 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8114316078192855e+89 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.811431607819288e+89 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8114316078192855e+89 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.811431607819288e+89 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8114316078192855e+89 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.811431607819288e+89 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8114316078192855e+89 | Val Accuracy = 0.10077777777777777\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.10077777777777777"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9c4PBcWq5otv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 884
        },
        "outputId": "a9de7ae8-5cf4-4e1c-a203-abbe4ce95e56"
      },
      "source": [
        "lr = 1e4\n",
        "Lambda = 1e-7\n",
        "train_and_test_loop(50, lr, Lambda)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss = 1.5107616570515062e+70 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8633505310350777e+70 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.8633505310350716e+70 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8633505310350777e+70 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.8633505310350716e+70 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8633505310350777e+70 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.8633505310350716e+70 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8633505310350777e+70 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.8633505310350716e+70 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8633505310350777e+70 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.8633505310350716e+70 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8633505310350777e+70 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.8633505310350716e+70 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8633505310350777e+70 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.8633505310350716e+70 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8633505310350777e+70 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.8633505310350716e+70 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8633505310350777e+70 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.8633505310350716e+70 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8633505310350777e+70 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.8633505310350716e+70 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8633505310350777e+70 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.8633505310350716e+70 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8633505310350777e+70 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.8633505310350716e+70 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8633505310350777e+70 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.8633505310350716e+70 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8633505310350777e+70 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.8633505310350716e+70 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8633505310350777e+70 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.8633505310350716e+70 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8633505310350777e+70 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.8633505310350716e+70 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8633505310350777e+70 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.8633505310350716e+70 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8633505310350777e+70 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.8633505310350716e+70 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8633505310350777e+70 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.8633505310350716e+70 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8633505310350777e+70 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.8633505310350716e+70 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8633505310350777e+70 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.8633505310350716e+70 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8633505310350777e+70 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.8633505310350716e+70 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8633505310350777e+70 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.8633505310350716e+70 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8633505310350777e+70 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.8633505310350716e+70 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8633505310350777e+70 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.8633505310350716e+70 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8633505310350777e+70 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.8633505310350716e+70 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8633505310350777e+70 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.8633505310350716e+70 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8633505310350777e+70 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.8633505310350716e+70 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8633505310350777e+70 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.8633505310350716e+70 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8633505310350777e+70 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.8633505310350716e+70 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8633505310350777e+70 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.8633505310350716e+70 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8633505310350777e+70 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.8633505310350716e+70 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8633505310350777e+70 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.8633505310350716e+70 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8633505310350777e+70 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.8633505310350716e+70 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8633505310350777e+70 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.8633505310350716e+70 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8633505310350777e+70 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.8633505310350716e+70 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8633505310350777e+70 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.8633505310350716e+70 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8633505310350777e+70 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.8633505310350716e+70 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8633505310350777e+70 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.8633505310350716e+70 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8633505310350777e+70 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.8633505310350716e+70 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8633505310350777e+70 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.8633505310350716e+70 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8633505310350777e+70 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.8633505310350716e+70 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8633505310350777e+70 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.8633505310350716e+70 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8633505310350777e+70 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.8633505310350716e+70 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8633505310350777e+70 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.8633505310350716e+70 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8633505310350777e+70 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.8633505310350716e+70 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8633505310350777e+70 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.8633505310350716e+70 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8633505310350777e+70 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.8633505310350716e+70 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8633505310350777e+70 | Val Accuracy = 0.10077777777777777\n",
            "Loss = 1.8633505310350716e+70 | Training Accuracy = 0.09966666666666667 | Val Loss = 1.8633505310350777e+70 | Val Accuracy = 0.10077777777777777\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.10077777777777777"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0AZatqbXCZ9O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}